---
title: "Data Mining Project - Analysis of Airbnb Rentals"
author: Tara Cool
output:
  html_notebook: default
  pdf_document: default
---


# Introduction

New York City's Airbnb rental market is a dynamic and diverse ecosystem, attracting global travelers and property owners. This data mining project delves into this complex landscape to extract valuable insights that inform decision-making for various stakeholders.

**Data Preparation & Exploration**

The project commences with a comprehensive Data Preparation & Exploration phase, involving the identification and treatment of missing values, handling of outliers, computation of summary statistics, and the creation of insightful data visualizations. These visualizations illuminate pricing trends, accommodation preferences, correlations, categorical listing characteristics, and host verification, providing stakeholders with valuable insights into the Airbnb market's intricacies.

**Prediction**

Subsequently, the project advances to the Prediction phase, utilizing multiple regression techniques to forecast Airbnb rental prices in New York City. The model incorporates diverse factors, including accommodation capacity and location, to offer pricing insights beneficial for both hosts and guests. While the model explains a significant portion of price variability, it also highlights unaccounted-for factors, prompting further exploration and refinement.

**Classification**

In the Classification phase, the project harnesses Naive Bayes classification to categorize rental prices into distinct bins, revealing pricing categories catering to diverse preferences and budgets. This classification extends to predicting cancellation policies, shedding light on the factors influencing these decisions for both hosts and guests.

**Clustering - Comparison of Neighborhoods in NYC**

Concluding the project, we explore Brooklyn's neighborhoods through k-Means clustering. By analyzing various features, we identify distinct clusters characterized by unique attributes. This analysis aids property investors, tourists, and urban planners in data-driven decision-making for Brooklyn's neighborhoods.

In summary, this data mining project offers a comprehensive exploration of the Airbnb rental market in New York City, supported by data visualizations that provide stakeholders with a clear understanding of market dynamics. It serves as a blueprint for similar analyses in bustling urban areas worldwide.*





```{r}
# Load required packages
library(scales)
library(data.table)
library(stats)
library(ggplot2)
library(dplyr)
library(forcats)
library(forecast)
library(caret)
library(tidyverse)
library(FNN)
library(ggcorrplot)
library(e1071)
library(rpart)
library(rpart.plot)
library(factoextra)
library(ggpubr)
library(pROC)
```


# **1. Data Preparation & Exploration**

```{r}
# Read data into local environment
df <- read.csv("train.csv")
```


```{r}
# Subset data frame to include only those records pertaining to NYC
ny.df <- df[df$city == "NYC", ]
```



## **a. Handling Missing Values/Data Pre-Processing**

```{r}
# Convert blank cells to 'NA's
ny.df[ny.df == ""] <- NA

# Calculate number of 'NA' values in data frame
sum(is.na(ny.df))  
```


```{r}
# Get percentage of rows that are "complete cases" (i.e., not missing values) 
percent(sum(complete.cases(ny.df))/nrow(ny.df), accuracy = 0.01)
```


Evaluate count/proportion of ‘NA’ values corresponding to each variable to understand the potential impact of dropping/imputing these missing values:
```{r}
# Subset columns that contain "NA" values & get count of NA values in each column 
num_NAs <- colSums(is.na(ny.df))  

# Compute percent of NA values in each column
prop_NAs <- percent(num_NAs/nrow(ny.df), accuracy = 0.01)  

# Create df to store missing value counts for each variable
var_NAs.df <- data.frame(num_NAs, prop_NAs)

# Convert to table
var_NAs <- data.table(var_NAs.df, keep.rownames = TRUE) 
colnames(var_NAs) <- c("Variable", "Num. of NAs", "% NAs")

var_NAs  # print table
```

*These insights will guide our process in deciding how we will handle NA values (e.g., drop or impute) for each data mining task (e.g., prediction, classification, & clustering)*


```{r}
# Handle missing values for numerical variables by imputing with median

# Convert 'host_response_rate' from character value to numerical
ny.df$host_response_rate <- as.numeric(sub("%", " ", ny.df$host_response_rate))

# Impute for missing values with median
ny.df$bathrooms[is.na(ny.df$bathrooms)] <- median(ny.df$bathrooms, na.rm = TRUE)
ny.df$host_response_rate[is.na(ny.df$host_response_rate)] <- median(ny.df$host_response_rate, na.rm = TRUE)
ny.df$review_scores_rating[is.na(ny.df$review_scores_rating)] <- median(ny.df$review_scores_rating, na.rm = TRUE)
ny.df$bedrooms[is.na(ny.df$bedrooms)] <- median(ny.df$bedrooms, na.rm = TRUE)
ny.df$beds[is.na(ny.df$beds)] <- median(ny.df$beds, na.rm = TRUE)
```

*The decision to impute missing values for numerical variables, such as 'host_response_rate,' 'bathrooms,' 'review_scores_rating,' 'bedrooms,' and 'beds,' with their respective medians serves the purpose of preserving the data's central tendencies while reducing the potential impact of outliers.*


*In summary, during this phase of data preparation and exploration, we identify and address missing values, including those within empty cells. Imputing missing values with median values is a chosen approach to maintain data integrity while taking outliers into account. This comprehensive approach guarantees data quality and completeness, laying a strong foundation for effective data mining tasks.*



```{r}
# Drop NA values for categorical variables that were not imputed
ny.df <- na.omit(ny.df)
```


```{r}
# Subset values not equal to 0, as 0 is unrealistic for many numeric variables
ny.df <- ny.df %>% filter(log_price > 0)
ny.df <- ny.df %>% filter(accommodates > 0)
ny.df <- ny.df %>% filter(bathrooms > 0)
ny.df <- ny.df %>% filter(number_of_reviews > 0)
ny.df <- ny.df %>% filter(review_scores_rating > 0)
ny.df <- ny.df %>% filter(bedrooms > 0)
```


```{r}
# Convert 'host_response_rate' & 'review_scores_rating' to decimal values that denote percentages
ny.df$host_response_rate <- round((ny.df$host_response_rate)/100, 2)
ny.df$review_scores_rating <- round((ny.df$review_scores_rating)/100, 2)
```


```{r}
# Subset of valid 'zipcode' values
valid_zipcode <- nchar(ny.df$zipcode) == 5

# Filter dataframe to keep only those rows with valid zipcodes
ny.df <- ny.df[valid_zipcode, ]
```


```{r}
# Convert 't'/'f' to 'True'/'False'
ny.df$host_identity_verified <- factor(ny.df$host_identity_verified, levels = c("t", "f"), labels = c("True", "False"))
ny.df$instant_bookable <- factor(ny.df$instant_bookable, levels = c("t", "f"), labels = c("True", "False"))
```


### **Feature Engineering**

*In this section, we will simplify several categorical variables by minimizing the number of categories/levels for variables like 'bed_type' & 'cancellation_policy.' We will also define several new variables using a combination of the existing variables in the dataset & external data. In particular, we will define the variables 'borough', 'amenities_list', & 'amenities_count'.*

Simplify categorical variables:
```{r}
# Group less common levels of 'bed_type' into a single column called 'Other' (keep only the most frequently occurring type of bed)
ny.df$bed_type <- fct_lump(ny.df$bed_type, 1)

# Combine levels 'super_strict_30' & 'super_strict_60' from the variable 'cancellation_policy' & creating new category called 'super_strict'
ny.df$cancellation_policy <- fct_other(ny.df$cancellation_policy, keep = c("flexible", "moderate", "strict"), other_level = "super_strict")
```


Define new variable 'borough':
```{r}
# Import dataset for mapping zipcodes to boroughs
borough_df <- read.csv("nyc_zip_borough_neighborhoods_pop.csv")

# Inspect dataset
str(borough_df)
```
Note: The dataset with information about the zipcode to borough mappings was downloaded/imported from the link below ...
[Zipcode to NYC Borough Mappings Dataset](https://data.beta.nyc/dataset/pediacities-nyc-neighborhoods/resource/7caac650-d082-4aea-9f9b-3681d568e8a5)


```{r}
# Subset necessary variables
borough_zip_df <- borough_df[, c("zip", "borough")]

# Convert 'zip' to character type
borough_zip_df$zip <- as.character(borough_zip_df$zip)

# Merge new dataset with Airbnb dataframe based on 'zipcode' variable
ny.df <- merge(ny.df, borough_zip_df, by.x = "zipcode", by.y = "zip", all.x = TRUE)

# Replace missing 'borough' values with "Other"
ny.df$borough[is.na(ny.df$borough)] <- "Other"

# Drop 'zipcode' from dataset
ny.df <- subset(ny.df, select = -c(zipcode))
```

In the steps above, we merge the two dataframes, 'ny.df_reg' & 'borough_zip_df,' based on a common 'zipcode'/'zip' column. Only the 'borough' column from 'borough_zip_df' is added to 'ny.df_reg.' If there are missing 'borough' values in 'ny.df_reg,' they are replaced with "Other." Finally, the 'zipcode' column is dropped from 'ny.df_reg' since it's no longer needed after the merge.


Define new variables 'amenities_list' & 'amenities_count':
```{r}
# Define list of amenities & count of amenities for each listing
ny.df <- ny.df %>%
  mutate(amenities_list = strsplit(amenities, ",")) %>%
  mutate(amenities_count = lengths(amenities_list))

# Drop now redundant 'amenities_list' variable
ny.df <- subset(ny.df, select = -c(amenities_list))
```


## **b. Summary Statistics**

```{r}
# Drop NA values for categorical variables that were not imputed
ny.df <- na.omit(ny.df)
```


Identify numeric & categorical variables:
```{r}
# Subset numeric columns
num_var <- ny.df[, sapply(ny.df, is.numeric)]

# Subset remaining (categorical) columns
cat_var <- ny.df[, !(names(ny.df) %in% names(num_var))]

# Drop unique identifier columns from subsets
num_var <- subset(num_var, select = -c(id))
cat_var <- subset(cat_var, select = -c(description, name, thumbnail_url))
```


```{r}
# Get summary stats for numerical variables
summary(num_var)
```


```{r}
# Standard deviation of each variable to better understand its overall distribution
options(scipen = 999)
col.sd <- apply(num_var, 2, sd)
col.sd  # print 'sd' object (vector of sd values for each column)
```


```{r}
# Interquartile range for numerical variables
col.iqr <- apply(num_var, 2, IQR)
col.iqr
```


```{r}
# compute variance of numerical variables
col.var <- apply(num_var, 2, var)
col.var
```


```{r}
# Create correlation matrix
corr <- round(cor(num_var), 2)
corr
```


```{r}
# Summarize ordinal categorical variables 

#generate the total number of observations belonging to each level
ordinal.cat.sum <- table(cat_var$cancellation_policy)

ordinal.cat.sum
```


```{r}
# Summarize nominal/binary nominal categorical variables 

#generate the total number of observations belonging to each class
nominal.cat.sum <- apply(subset(cat_var, select = - c(cancellation_policy, first_review, last_review, host_since, amenities)), 2, table)
nominal.cat.sum

# Note: some of the variables were removed here. For example, 'cancellation_policy' was excluded as it is an ordinal variable. Other variables such as 'first_review' and 'host_since' were removed due to the number of unique date values given for each of these variables. 
```




*The summary statistics and measures of dispersion we've calculated provide valuable insights that can guide decision-making for both Airbnb users and management. Let's delve deeper into how this information can be used:*

**1. Pricing Insights:**

  - *Average and Standard Deviation of Price*: By analyzing the average and standard deviation of prices, Airbnb users can get a sense of the typical price range for listings in New York City. Additionally, breaking down these statistics by room type (e.g., entire home, private room, shared room) allows users to understand which types of accommodations are more budget-friendly or luxurious.

**2. Accommodation Preferences:**

  - *Median Number of Bedrooms*: For travelers looking for more space, knowing the median number of bedrooms can help them identify listings that meet their requirements. On the management side, this information can guide property investments and renovations, ensuring that accommodations match market demand.

**3. Correlation Insights:**

  - *Positive Correlations*: Understanding strong positive correlations, such as between 'log_price' and 'accommodates,' can be valuable for Airbnb users. It suggests that as the number of people a property accommodates increases, so does the price. This information helps users make informed decisions when selecting properties based on their group size.
  - *Variable Reduction*: For Airbnb management, identifying correlated variables can aid in variable reduction for predictive modeling. By eliminating highly correlated predictors, they can build more efficient and interpretable models.

**4. Categorical Variables:**

  - *Categorization of Listings*: The summary of categorical variables, like 'property_type,' 'room_type,' and 'neighborhood,' provides insights into the diversity of Airbnb listings in NYC. This information helps users narrow down their choices based on their preferences and requirements.
  - *Host Verification*: Airbnb management can use the summary of 'host_identity_verified' to evaluate the trustworthiness of hosts, which can be a crucial factor in attracting guests.

**5. Decision Support for Airbnb Management:**

  - *Pricing Strategy*: Management can adjust pricing strategies based on the average price and standard deviation of prices. For example, they can offer promotional rates during low-demand seasons to attract more bookings.
  - *Property Investment*: Data on the median number of bedrooms can inform property investment decisions. If there's a high demand for larger accommodations, management may consider acquiring or developing properties with more bedrooms.
  - *Marketing and Targeting*: Understanding the popularity of room types or neighborhoods can help in marketing efforts. Management can target specific demographics or interests to increase occupancy rates in certain areas.

*In summary, these summary statistics go beyond mere data description; they empower Airbnb users to make informed booking decisions and offer valuable insights for management to optimize their property listings and pricing strategies. These insights can ultimately lead to improved guest experiences and increased revenue for hosts and Airbnb itself.*



## **c. Data Visualization**

**(1) Faceted Bar Chart**
```{r}
ggplot(ny.df, aes(x= room_type, fill = room_type)) + geom_bar(color = "black", alpha = 0.7) + labs(title = "Airbnb Rental Room Types in NYC by Cleaning Fee Policy", x = NULL, y = "# of Airbnb Rental Listings") + theme(axis.title = element_text(size = 12), legend.position = "bottom") + scale_x_discrete(labels = NULL) + facet_wrap(~cleaning_fee, labeller = labeller(cleaning_fee = c("True" = "With Cleaning Fee", "False" = "Without Cleaning Fee"))) + scale_fill_discrete(name = "Room Type") 
```


**(2) Histogram**
```{r}
ggplot(ny.df, aes(x= accommodates)) + geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) + labs(title = "Distribution of Accommodation Capacity in NYC Airbnb Rentals", x = "Number of Accomodated Guests", y= "# of Airbnb Rental Listings") + theme_minimal() + theme(axis.text = element_text(size = 11), axis.title = element_text(size = 12)) 
```


**(3) Heat Map/Correlation Matrix**
```{r}
library(ggcorrplot)

ggcorrplot(corr, lab = TRUE, lab_size = 2, title = "Correlation Heatmap of NYC\nAirbnb Rental Property Data") + theme(plot.title = element_text(size = 13), axis.text.x = element_text(size = 9), axis.text.y = element_text(size = 9))
```


**(4) Proportional Bar Chart**
```{r}
# Define new variable 'property_group' that groups property types
# (Goal = limit num. of levels)
ny.df$property_group <- ifelse(ny.df$property_type %in% c("Guesthouse", "Guest suite", "In-law"), "Guest suite/In-law", 
                               ifelse(ny.df$property_type %in% c("Boutique hotel", "Dorm", "Hostel", "Serviced apartment", "Timeshare"), "Accommodation",
                                      ifelse(ny.df$property_type %in% c("Boat", "Bungalow", "Cabin", "Castle", "Chalet", "Earth House", "Tent", "Vacation home", "Villa", "Yurt"), "Specialty", 
                                             as.character(ny.df$property_type))))
```


```{r}
# Create Stacked Bar Chart
ggplot(ny.df, aes(x= accommodates, fill = property_group)) + geom_bar(position = "fill", color = "black", alpha = 0.7) + labs(title = "Proportion of Airbnb Listings in NYC by Accommodation\nCapacity & Property Type", x = "Accommodation Capacity", y = "Proportion of Airbnb Listings") + scale_fill_discrete(name = "Property Type") + theme_minimal() +  theme(axis.text = element_text(size = 10), axis.title = element_text(size = 11)) 
```


**(5) Histogram #2**
```{r}
ggplot(ny.df, aes(x= log_price)) + geom_histogram(binwidth = 1, fill = "orange", color = "black", alpha = 0.7) + labs(title = "Distribution of Log Prices for NYC Airbnb Rentals", x = "Log Price ($)", y= "# of Airbnb Rental Listings") + theme_minimal() + theme(axis.text = element_text(size = 11), axis.title = element_text(size = 12)) 
```


**(6) Scatterplot**
```{r}
ggplot(ny.df, aes(x= bedrooms, y= log_price, color = room_type, size = accommodates)) + geom_point(na.rm= TRUE, alpha = 0.7) + xlim(1, 8) + labs(title= "Number of Bedrooms vs. Log Price for NYC Airbnb Rentals, by Room Type & Accommodation Capacity", x= "# of Bedrooms", y= "Log Price ($)") + scale_color_discrete(name = "Room Type") + scale_size_continuous(name = "Accommodation Capacity") + theme_minimal() + theme(axis.text = element_text(size = 11), axis.title = element_text(size = 12), legend.text = element_text(size = 10)) 
```


*In this crucial phase of the data preparation and exploration process, R and the adaptable 'ggplot2' library within R/R Studio were employed to uncover profound insight; empowering both Airbnb users and management to make well-informed decisions and optimizing their experience in the dynamic New York City market.*

**1. Faceted Bar Chart: Understanding Cleaning Fee Policies**

Our first visualization is a faceted bar chart that meticulously dissects Airbnb rental room types in NYC based on their cleaning fee policies. By segmenting the data in this manner, we reveal valuable insights that can help both hosts and guests. For Airbnb management, this information can aid in setting competitive pricing strategies and policies for different room types. Guests can benefit from this knowledge by making more informed decisions about accommodation based on their preferences and budget. This plot uncovers that cleaning fees are prevalent, especially for 'entire home/apartment' listings. Such insights can guide both hosts and guests in negotiations and bookings.

**2. Histogram: Revealing Accommodation Preferences**

The second visualization, a histogram showcasing accommodation capacity distribution, lays bare the preferences of Airbnb renters in New York City. For hosts, this is a goldmine of information, allowing them to tailor their listings to the most sought-after capacities, thereby optimizing occupancy rates and revenue. For guests, this histogram is a powerful tool for finding the perfect match based on group size. It shows that most listings can comfortably host around 2 guests, but it also highlights the availability of properties for larger groups. This revelation aids in decision-making for both hosts and guests.

**3. Heat Map/Correlation Matrix: Unveiling Insights from Data**

Our third visualization, the correlation heatmap, delivers a deeper understanding of how various numerical variables relate to one another. For decision-makers in the Airbnb ecosystem, this plot offers predictive potential. Strong correlations between 'beds,' 'bedrooms,' and 'accommodates' with 'log_price' can be valuable for pricing optimization. Meanwhile, the negative correlation between latitude and 'log_price' suggests that location significantly influences rental prices. Hosts can set competitive prices, and guests can better assess property values based on this knowledge.

**4. Proportional Bar Chart: Navigating Property Types and Capacity**

The fourth visualization, a stacked bar chart, guides Airbnb management and users in understanding the distribution of property types and their capacity. This chart is an invaluable resource for hosts to fine-tune their listings based on property type and group size. Apartments emerge as the dominant choice, particularly for smaller parties. Houses, on the other hand, become more appealing for larger groups. Airbnb users can capitalize on this knowledge to make well-informed booking decisions.

**5. Histogram #2: Unearthing Pricing Patterns**

Our fifth visualization is another histogram, this time focusing on the distribution of log-transformed prices. This transformed scale can unveil hidden pricing trends or clusters that are not immediately apparent. For both hosts and guests, this histogram offers deeper insights into the nuanced price dynamics of Airbnb rentals in NYC.

**6. Scatterplot: Understanding Price Dynamics**

Lastly, our sixth plot, a scatterplot, delves into the intricate relationship between several variables: number of bedrooms, log-transformed prices, room types, and accommodation capacity. This visualization empowers both hosts and guests to decipher how these factors interact and influence rental prices. Notably, it sheds light on the price variations tied to room type and capacity, offering actionable insights for optimizing pricing strategies and booking decisions.

Together, these insightful visualizations equip Airbnb stakeholders with a wealth of information, enabling them to make data-driven decisions that enhance the Airbnb experience in New York City. Whether you're a host seeking to maximize revenue or a guest in pursuit of the perfect stay, these visualizations are your compass in navigating the NYC Airbnb landscape.





# **2. Prediction**

## **Multiple Regression Model for Predicting Airbnb Rental Prices in NYC**

*In this portion of the project, we aim to predict Airbnb rental prices in New York City using a multiple regression model. Through data preprocessing, variable selection, and model refinement, we'll uncover key insights that benefit both hosts and guests in this bustling metropolis.*

### **Data Pre-processing & Variable Selection**

```{r}
# Variable/dimension reduction
ny.df_reg <- subset(ny.df, select = -c(city, description, first_review, host_has_profile_pic, host_since, id, last_review, name, neighbourhood, property_type, thumbnail_url, beds, amenities))  
```

Note: The decisions to remove the aforementioned variables from the dataset are described below:
* 'city': all observations in this subset of the original dataset contain only those observations where the value for the 'city' column is "NYC." 
* 'zipcode' & 'neighbourhood': these variables are redundant in describing the location of a listing (e.g., 'latitude', 'longitude', & 'neighbourhood' are more detailed/precise variables). 
* 'id', 'name', & 'description': the values for each of these variables are unique to each observation. 
* 'property_type': this variable is now redundant, as we created a new variable for property types that minimizes the number of categories (i.e., grouped less common types into larger categories).
* 'host_has_profile_pic': as 'summary()' function implies that in a majority of the rental listings, the host has a profile picture (e.g., 32,076 = True & 97 = False). 
* 'first_review', 'last_review' & 'host_since': the relevance of these variables could be limited to assessing the recent performance and maintenance of a listing or the performance of the host, but might not directly affect the pricing decisions.
* 'thumbnail_url': unlikely to influence the price of an Airbnb listing, as it is typically a web link to an image, and its inclusion in the model would not offer any meaningful insights into pricing. 
* 'beds': this variable is redundant as we have the variables 'bedroom' & 'accommodates'.
* 'amenities': this variable is redundant in describing the amenities of each Airbnb listing. A better way to quantify this in our model is through the use of the 'amenities_count' variable.



### **Partitioning of Data - Training & Validation Sets**
```{r}
# Partition the data into training (60%) & validation (40%) sets
set.seed(1)

# Sample 60% of the data, which we will assign to the training data set
train.index <- sample(c(1:nrow(ny.df_reg)), nrow(ny.df_reg)*0.6)  

# Assign 60% of the data that we just sampled to training set
train.df <- ny.df_reg[train.index, ]  

# Assign remaining 40% of the data to validation set
valid.df <- ny.df_reg[-train.index, ] 
```


### **Examine Bivariate Relationships**

Identify numeric & categorical predictor variables:
```{r}
# Subset numeric columns from training set while excluding 'log_price'
num_predictors <- train.df[, !(names(train.df) %in% 'log_price') & sapply(train.df, is.numeric)]

# Subset remaining (categorical) columns from training set while excluding 'log_price'
cat_predictors <- train.df[, !(names(train.df) %in% c(names(num_predictors), 'log_price'))]
```


Check for multicollinearity issues:
```{r}
# Calculate the correlation matrix with numeric variables
reg_cor_matrix <- cor(train.df[sapply(train.df, is.numeric)])

# Visualize correlation matrix
ggcorrplot(reg_cor_matrix, lab = TRUE, lab_size = 2.5, title = "Correlation Heatmap of NYC Airbnb\nRental Property Data") + theme(plot.title = element_text(size = 13), axis.text.x = element_text(size = 9), axis.text.y = element_text(size = 9))
```
Multicollinearity occurs when the input variables are highly correlated, making it challenging to distinguish the unique contribution of each variable to the model and decreasing the reliability of the model output. It should be noted that there are 2 input variables which are strongly correlated with one another - including 'bedrooms' and 'accommodates'.

Nonetheless, the criterion for selecting variables to drop in the revised model was based on both their correlation with the output variable 'log_price,' as well as their correlation with each other. Given that 'accommodates' has a stronger correlation to 'log_price' than 'bedrooms', it might be wise to keep the accommodates variable and drop the 'bedrooms' variable in an effort to avoid multicollinearity issues. This choice ensures that we retain the most influential variables while eliminating unnecessary redundancy in the input features, ultimately improving the model's performance and robustness. 


### **Initial MLR Model**
```{r}
# Run MLR of 'log_price' on all the predictors in the training set

# Note: all binary nominal categorical variables will automatically be converted into dummy variables with 'm-1' dummies

mlr.model <- lm(log_price ~ ., data = train.df)
options(digits = 3, scipen = 999)
mlr_summary <- summary(mlr.model)
mlr_summary
```


```{r}
# Summary of residuals for initial MLR model (training set)
summary(mlr.model$residuals)
```


```{r}
# Assess accuracy of initial model against training set
accuracy(mlr.model$fitted.values, train.df$log_price)
```


In an effort to improve the predictive accuracy of the model, we will further refine the model by eliminating predictor variables in which the resulting p-value is greater than 0.05 -- suggesting those specific predictor variables are not linearly related to the output variable of 'log_price' when controlling for other variables. As such, we will drop predictor variables such as 'bed_type', 'host_identity_verified', and 'host_response_rate' from the model in which the p-value is greater than 0.05. We will also drop the 'bedrooms' variable that is strongly correlated with the 'accommodates' variable to evaluate the potential impact of multicollinearity on the model's predictive accuracy. However, we will keep some of the categorical variables where many of the categories for these variables are significant (e.g., 'borough' & 'cancellation_policy').
```{r}
# Drop insignificant predictor variables
train.df2 <- subset(train.df, select = -c(bed_type, host_response_rate, host_identity_verified, bedrooms))
```


```{r}
# Refined MLR model
mlr.model.2 <- lm(log_price ~ ., data = train.df2)
summary(mlr.model.2)
```


```{r}
# Assess accuracy of refined model against training set
accuracy(mlr.model.2$fitted.values, train.df$log_price)
```


*Stepwise Regression*
```{r}
# Apply stepwise regression, which drops predictors that lack statistical significance from the intial MLR model in an effort to determine the best subset of predictor variables 
mlr.model.step <- step(mlr.model, direction = "both")
summary(mlr.model.step)
```
After running stepwise regression on the initial model, 'bed_type' was the only variable dropped from the model. 


```{r}
# Assess accuracy of initial model after applying stepwise regression 
accuracy(mlr.model.step$fitted.values, train.df$log_price)
```


* The initial model with both 'Accommodates' and 'Bedrooms' has a slightly higher adjusted R-squared value, indicating that it explains a bit more of the variance in 'log_price' compared to the refined model.

* The RMSE of the initial model is slightly lower, which means it has slightly better predictive accuracy in terms of the error between predicted and actual 'log_price' values (i.e., initial model may provide a slightly better fit to the data for prediction purposes).


### **Model Evaluation**
```{r}
# Fitting MLR model to validation data & measuring model accuracy
library (forecast)  # load 'forecast' package for predictions

# Initial model
mlr.pred <- predict(mlr.model, newdata= valid.df)
accuracy(mlr.pred, valid.df$log_price)
```


```{r}
# Refined model
mlr.2.pred <- predict(mlr.model.2, newdata= valid.df)
accuracy(mlr.2.pred, valid.df$log_price)
```


```{r}
# Intial model + Stepwise regression
mlr.step.pred <- predict(mlr.model.step, newdata= valid.df)
accuracy(mlr.step.pred, valid.df$log_price)
```


```{r}
all.residuals <- valid.df$log_price - mlr.step.pred
hist(all.residuals, breaks = 25, xlab = "Residuals", main = " ")
```
Based on the histogram of residual errors when the model is fit to the validation data, one can conclude that most errors are between -1 and 1 (i.e., error magnitude).


### **Summary of MLR Modeling Process:**
In this phase of the data mining project, the primary objective was to construct a robust multiple linear regression (MLR) model capable of predicting the logarithm of rental property prices ('log_price') on Airbnb listings in New York City. The modeling process was characterized by careful consideration of various factors to strike a balance between predictive accuracy and model interpretability, ensuring the final model's reliability.

The process began with thorough data preprocessing, which included addressing missing values and filtering out any data points that appeared unrealistic. This rigorous data preparation was fundamental to ensuring the model's quality.

One of the pivotal decisions in variable selection was tackling multicollinearity among predictor variables. A strong correlation was observed between 'bedrooms' and 'accommodates.' To maintain model stability and interpretability, 'accommodates' was retained, given its stronger correlation with 'log_price.' Additionally, certain categorical variables like 'bed_type' and 'cancellation_policy' were streamlined to enhance the model's efficiency.

The stepwise regression technique was employed to further refine the model by eliminating variables with p-values exceeding 0.05, indicating a lack of a significant linear relationship with 'log_price.' However, certain categorical variables such as 'borough' and 'cancellation_policy' were judiciously retained due to their demonstrated statistical significance.

The overarching goal throughout this process was to create a parsimonious yet accurate MLR model. After comparing two models—one with both 'bedrooms' and 'accommodates' and another with only 'accommodates'—it was concluded that the initial model, which included both variables, provided a slightly better fit to the data for prediction purposes. Therefore, the initial model was selected as the preferred choice, aligning with the primary aim of maximizing predictive accuracy.

In summary, the modeling process was a thoughtful and meticulous journey, guided by a commitment to optimizing the predictive performance of the MLR model while maintaining its interpretability and reliability. The selected model, with 'bedrooms' and 'accommodates' as predictors, serves as a robust tool for predicting Airbnb listing prices in the vibrant city of New York.


### **Key Findings:**
* The adjusted R-squared value of our initial model is 0.696, indicating that approximately 69.6% of the variability in 'log_price' can be explained by the selected independent variables. This suggests that our model provides a reasonably good fit to the data. However, it also implies that there are other factors not accounted for in our model that influence rental prices, such as seasonal variations, special events, or unique property features.

* The Root Mean Square Error (RMSE) of our model is 0.362. This metric quantifies the average magnitude of prediction errors. In our context, an RMSE of 0.362 means that, on average, our model's predictions of log_price deviate from the actual values by approximately 0.362 units. This level of accuracy is promising, considering the logarithmic transformation of the response variable. 

* When fitting the refined model to the validation set, the RMSE remained consistent at 0.362. This indicates that our model's predictive performance generalizes well to new, unseen data, assuring its reliability in providing consistent insights for both hosts and guests in the dynamic Airbnb rental market of New York City.

In conclusion, the multiple regression model provides valuable insights into the factors influencing Airbnb rental prices in New York City. It suggests that accommodation capacity, the number of available amenities, location, and past ratings/reviews significantly impact rental prices. The model explains a substantial portion of the price variability, but further research could explore additional factors to enhance predictive accuracy. The insights obtained from the model output highlight the importance of considering these factors when setting rental prices on the Airbnb platform, ultimately aiding both hosts and guests in making informed decisions.





# **3. Classification**

## **a. k-Nearest Neighbors**

### **Data Pre-processing & Partitioning**
```{r}
# Convert the predictive outcome of 'cleaning_fee' into a factor
ny_k.df <- ny.df
ny_k.df$cleaning_fee <- as.factor(ny.df$cleaning_fee)
str(ny_k.df$cleaning_fee)
```


```{r}
# Remove columns not used as predictors (i.e., variables not relevant to cleaning fees)
ny_k.df <- subset(ny_k.df, select = -c(id, amenities, bed_type, city, description, first_review, host_has_profile_pic, host_identity_verified, host_response_rate, host_since, last_review, name, neighbourhood, thumbnail_url, property_group))

summary(ny_k.df)
```


```{r}
# Set seed with value 60 & partition the dataset into training (60%) & validation (40%) sets
set.seed(60)  # Set the seed here
ny_k.df_train.index <- sample(c(1:nrow(ny_k.df)), nrow(ny_k.df) * 0.6)
ny_k_train.df <- ny_k.df[ny_k.df_train.index, ]
ny_k_valid.df <- ny_k.df[-ny_k.df_train.index, ]
```


### **Separate Rentals**
```{r}
# Separate the rentals with/without a cleaning fee in training set
train.df_t <- subset(ny_k_train.df, cleaning_fee == "True")
train.df_f <- subset(ny_k_train.df, cleaning_fee == "False")
```


### **Examine Differences in Mean Values**
```{r}
# Examine the percentage difference in the mean value among the numeric predictor variables
(mean(train.df_t$log_price) - mean(train.df_f$log_price)) * 100
(mean(train.df_t$accommodates) - mean(train.df_f$accommodates)) * 100
(mean(train.df_t$bathrooms) - mean(train.df_f$bathrooms)) * 100
(mean(train.df_t$bedrooms) - mean(train.df_f$bedrooms)) * 100
(mean(train.df_t$beds) - mean(train.df_f$beds)) * 100
```


### **Variable Selection**
```{r}
# If any variables are categorical or show less than 10% difference in mean value between the two groups, remove those variables entirely
ny_k_train.df <- subset(ny_k_train.df, select = -c(bathrooms, property_type, room_type, cancellation_policy, borough, instant_bookable))
ny_k_valid.df <- subset(ny_k_valid.df, select = -c(bathrooms, property_type, room_type, cancellation_policy, borough, instant_bookable))
ny_k.df <- subset(ny_k.df, select = -c(bathrooms, property_type, room_type, cancellation_policy, borough, instant_bookable))
str(ny_k_train.df)
```


### **Normalization**
```{r}
# Normalize the data using the training set & 'preProcess()' function.
library(caret)  # Load the caret library
train.norm.df <- ny_k_train.df
valid.norm.df <- ny_k_valid.df
ny_k.norm.df <- ny_k.df

# Specify the columns to normalize
columns_to_normalize <- c("log_price", "accommodates", "bedrooms", "beds")

# Create a preProcess object
norm_values <- preProcess(ny_k_train.df[, columns_to_normalize], method = c("center", "scale"))

# Apply normalization to the training and validation data
train.norm.df[, columns_to_normalize] <- predict(norm_values, ny_k_train.df[, columns_to_normalize])
valid.norm.df[, columns_to_normalize] <- predict(norm_values, ny_k_valid.df[, columns_to_normalize])
ny_k.norm.df[, columns_to_normalize] <- predict(norm_values, ny_k.df[, columns_to_normalize])
```


### **Create New Rental**
```{r}
# Make up a new rental to predict/classify the cleaning fee to train the model
new.df <- data.frame(log_price = 4, accommodates = 5, bedrooms = 4, beds = 5)

# Ensure that the columns in new.df match the columns used for normalization in the training data
new.df[, columns_to_normalize] <- predict(norm_values, new.df[, columns_to_normalize])
```


### **k-nn Model Evaluation**
```{r}
# Using the validation data and a range of k values from 1 to 14, access the accuracy level for each k value from 1 to 14

# Initialize a data frame with two columns: k, & accuracy
accuracy.df <- data.frame(k = seq(1, 14, 1), accuracy = rep(0, 14))

# Compute the accuracy level for each k value & find the optimal k-value
for (i in 1:14) {
  knn.pred <- knn(train.norm.df[, columns_to_normalize], valid.norm.df[, columns_to_normalize], cl = train.norm.df[, "cleaning_fee"], k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, valid.norm.df[, "cleaning_fee"])$overall[1]
}

accuracy.df
```


### **k-nn Model Prediction**
```{r}
# Using the knn() function, the normalized training data, & the optimal k=11, generate a predicted classification of cleaning_fee for the new rental.
optimal_k <- which.max(accuracy.df$accuracy)
optimal_k_value <- accuracy.df$k[optimal_k]

nn <- knn(train = train.norm.df[, columns_to_normalize], test = new.df[, columns_to_normalize], cl = train.norm.df[, "cleaning_fee"], k = optimal_k_value)
predicted_cleaning_fee <- as.character(nn)
predicted_cleaning_fee
```

The prediction is 'True' - the fictional NYC Airbnb rental will have a cleaning fee.





### **Explanation of k-NN Model**
In the third part of the data mining project, a k-nearest neighbors (k-NN) classification model was implemented to predict whether or not an Airbnb rental in New York City would include a cleaning fee. The construction of this predictive model involved several systematic steps to ensure its reliability and accuracy.

To begin, the dataset was preprocessed by transforming the 'cleaning_fee' variable into a factor, representing the presence or absence of cleaning fees. Subsequently, irrelevant columns, such as URLs and non-predictive attributes, were removed from the dataset. Missing values were also handled by eliminating rows with any NA values, as k-NN models do not accommodate missing data.

To establish a robust model, the dataset was split into training and validation sets using a 60-40 partition while maintaining reproducibility through the application of a random seed. Within the training dataset, a comparative analysis of mean differences between rentals with and without cleaning fees was conducted for various predictor variables. This allowed for the identification of attributes that significantly contributed to the classification task. Variables demonstrating minimal differences or being categorical in nature were excluded from consideration to prevent potential similarity bias.

Normalization of the data was imperative to ensure that all predictor variables contributed equally to the model. The 'preProcess' function from the 'caret' package was employed to standardize the data, rendering it suitable for k-NN classification.

Subsequently, k-NN classification was performed on the validation dataset, with k values ranging from 1 to 14. Model accuracy was evaluated for each k value, and it was determined that the optimal k-value was 11, resulting in an accuracy rate of 73.3%.

Finally, the k-NN model with the optimal k-value was applied to predict whether a fictitious rental, characterized by specific attributes (log_price = 4, accommodates = 5, bedrooms = 4, beds = 5), would include a cleaning fee. The model produced a prediction of 'True,' indicating that the new rental was likely to have a cleaning fee.

In this instance, it is vital to address and safeguard the model against similarity bias. Similarity bias occurs when the model assigns similar instances to the same class without adequately considering individual attribute importance. This can lead to misclassification, particularly when variables exhibit strong correlations or when categorical variables are not treated with appropriate consideration. The removal of variables with minimal class differences and categorical attributes aimed to mitigate similarity bias, ensuring the model's accuracy and fairness in classifying cleaning fees for Airbnb rentals in New York City.





## **b. Naive Bayes Classifier**

*In this section of our project, we employ the Naive Bayes algorithm to categorize NYC rental prices into four equally-sized bins: "Pricey Digs," "Above Average," "Below Average," and "Student Budget." This technique utilizes five carefully chosen predictors to make accurate predictions about price categories. The process involves rigorous model selection, training, and validation to ensure robust results.*

### **Data Pre-processing**

*Using the log_price variable, we create 4 similarly-sized bins, or categories, for the rental prices in NYC (Pricey Digs, Above Average, Below Average, Student Budget).*

```{r}
# Create copy of dataset & generate summary of 'log_price'
ny_nb.df<- ny.df
summary(ny_nb.df$log_price)
```


### **Binning the 'log_price' Variable**
```{r}
# Create bins for the 'log_price' variable
ny_nb.df$log_price <- cut(ny_nb.df$log_price, breaks=c(0.000, 4.248, 4.654, 5.165, 7.600), labels=c("Pricey Digs", "Above Average", "Below Average", "Student Budget"))

str(ny_nb.df$log_price)
```


### **Select Predictor Variables**
```{r}
# Subset necessary columns
ny_nb.df <- subset(ny_nb.df, select = c(log_price, accommodates, bedrooms, bathrooms, room_type, property_type))
```

Note: Five predictors variables were selected for model building: property_type, room_type, accommodates, bathrooms, bedrooms


### **Convert Numerical Variables to Categorical**
```{r}
# Convert numerical variables to categorical 
ny_nb.df$accommodates <- factor(ny_nb.df$accommodates)
ny_nb.df$bathrooms <- factor(ny_nb.df$bathrooms)
ny_nb.df$bedrooms <- factor(ny_nb.df$bedrooms)
```


### **Partition Dataset**
```{r}
# Partition dataset into training & validation sets
set.seed(60)
train_nb.index <- sample(c(1:dim(ny_nb.df)[1]), dim(ny_nb.df)[1]*0.6)
selected.var <- c(1, 2, 3, 4, 5, 6)
train_nb.df <- ny_nb.df[train_nb.index, selected.var]
valid_nb.df <- ny_nb.df[-train_nb.index, selected.var]
```


### **Naive Bayes Model**
```{r}
# Generate Naive Bayes model
ny_nb <- naiveBayes(log_price ~ ., data = train_nb.df)
ny_nb
```

The 'A-priori probabilities' given above denote the likelihood that an Airbnb listing in NYC belongs to each of these four classes. The likelihood of each class occuring in the training data is as follows:
* "Pricey Digs": 0.279
* "Above Average": 0.261
* "Below Average": 0.237
* "Student Budget": 0.224


The Naive Bayes classifier will use these probabilities to make predictions. Given a set of predictor variable values for an instance, the classifier will calculate the probability of the instance belonging to each class and assign it to the most likely class (the one with the highest probability).

To demonstrate, we will predict the price class for a fictional apartment with the following characteristics: 
* property_type = “Apartment”, 
* room_type = “Entire home/apt”, 
* accommodates= 4,
* bathrooms = 1, 
* bedrooms = 3


### **Predict Price Class for a Fictional Listing**
```{r}
# Predict probabilities & class membership for fictional listing
pred.prob <- predict(ny_nb, newdata = valid_nb.df, type = "raw")
pred.class <- predict(ny_nb, newdata = valid_nb.df)
df <- data.frame(actual = valid_nb.df$log_price, predicted = pred.class, pred.prob)
df[valid_nb.df$property_type == "Apartment" & valid_nb.df$room_type == "Entire home/apt" & valid_nb.df$accommodates == 4 &
valid_nb.df$bathrooms == 1 & valid_nb.df$bedrooms == 3,]
```


### **Confusion Matrix**
```{r}
# Training set
pred.class <- predict(ny_nb, newdata = train_nb.df)
confusionMatrix(pred.class, train_nb.df$log_price)

# Validation set
pred.class <- predict(ny_nb, newdata = valid_nb.df)
confusionMatrix(pred.class, valid_nb.df$log_price)
```


### **Summary of Naive Bayes Classifier:**

In this section of the project, we implemented the Naive Bayes algorithm to categorize Airbnb rental prices in New York City (NYC) into four distinct bins: "Pricey Digs," "Above Average," "Below Average," and "Student Budget." This categorization allows us to provide valuable insights for both Airbnb management and potential customers. The Naive Bayes model was developed using a subset of the original data for NYC that includes five carefully chosen predictor variables: property type, room type, accommodates (the number of guests the listing can accommodate), bathrooms, and bedrooms.

The first step was to create the price bins based on the 'log_price' variable. We split the prices into four categories, ensuring an approximately equal distribution of listings across these categories. The summary of the 'log_price' variable indicates that the rental prices in NYC range from 2.30 to 7.60, with a median of 4.61. After binning, we converted the numerical predictor variables ('accommodates,' 'bathrooms,' and 'bedrooms') into categorical variables to prepare them for modeling.

The Naive Bayes model was then trained on a subset of the dataset, with 60% of the data used for training and the remaining 40% for validation. The model's results are shown in the output, where it calculates conditional probabilities for each combination of predictor values in relation to the four price categories. 


### **Key Insights:**

Conditional probabilities for predictor variables like 'accommodates,' 'bedrooms,' 'bathrooms,' 'room_type,' and 'property_type' play a pivotal role in the Naive Bayes classifier. These probabilities indicate the likelihood of observing particular predictor variable values within a specific class. They are used to estimate the probability of a specific class given the observed predictor variable values, helping the classifier make predictions by identifying the most probable class based on the observed data.

The key insights from the model's conditional probabilities are as follows:

1. **Accommodation Capacity**: Listings that can accommodate fewer guests (e.g., 'accommodates' = 1-4) are more likely to fall into the "Pricey Digs" and "Above Average" categories. This suggests that smaller properties or those suitable for fewer people are associated with higher price categories. On the other hand, listings that accommodate more guests (e.g., 'accommodates' = 5-12) are more likely to be in the "Student Budget" category. This implies that larger properties or those suitable for more people are associated with lower price categories.

2. **Number of Bedrooms**: Listings with fewer bedrooms (e.g., 1 bedroom) are more likely to be in the "Pricey Digs" category, suggesting that smaller properties with fewer bedrooms tend to be in the higher price category. Conversely, listings with more bedrooms (e.g., 3-10 bedrooms) are more likely to be in the "Above Average," "Below Average," or "Student Budget" categories, indicating that larger properties with more bedrooms are associated with a range of price categories.

3. **Number of Bathrooms**: Listings with fewer bathrooms (e.g., 1 bathroom) are more likely to be in the "Pricey Digs" category, suggesting that properties with fewer bathrooms are associated with higher prices. By contrast, listings with more bathrooms (e.g., 2-5 bathrooms) are more likely to be in the "Above Average," "Below Average," or "Student Budget" categories, indicating that properties with more bathrooms are distributed across different price categories.

4. **Room Type**: Listings that offer an "Entire home/apt" are more likely to be in the "Pricey Digs" category, suggesting that entire homes or apartments tend to be in the higher price category. Contrarily, listings that offer a "Private room" are more likely to be in the "Above Average" category, indicating that private rooms are associated with a somewhat lower price category. Moreover, listings that offer a "Shared room" are more likely to be in the "Below Average" or "Student Budget" categories, implying that shared rooms are associated with lower price categories.

5. **Property Type**: Listings with an "Apartment" property type are more likely to be in the "Pricey Digs" category, suggesting that apartments tend to be in the higher price category. On the other hand, listings with a "Bed & Breakfast" property type are more likely to be in the "Above Average" category, indicating that bed & breakfast accommodations are associated with a somewhat lower price category. Furthermore, listings with a "Boat" property type are more likely to be in the "Below Average" or "Student Budget" categories, implying that boats are associated with lower price categories.

Furthermore, the model's performance was rigorously evaluated using confusion matrices for both the training and validation datasets. While accuracy is a useful metric, a deeper analysis of the results unveils both the model's potential and areas for enhancement. In the training set, the model achieved an accuracy of approximately 51.9%, and a similar accuracy of 50.7% in the validation set. However, accuracy alone may not provide a complete picture of the model's effectiveness.

The confusion matrices reveal important insights:

- **Sensitivity (True Positive Rate):** The model excels in correctly categorizing instances with 'Pricey Digs,' demonstrating a sensitivity of 87.6% in the validation set. This suggests that for high-priced listings, the model is quite reliable.

- **Specificity (True Negative Rate):** The model's specificity of 70.0% in the validation set for 'Above Average' listings indicates its ability to correctly identify cases where listings are not in this category.

- **Challenges in Classification:** The model faces difficulties in distinguishing between 'Above Average' and 'Below Average' listings, with sensitivity values of 8.34% and 49.5% respectively. This indicates that further improvements are needed in these areas.

While the model exhibits promise, it's important to acknowledge potential drawbacks and explore reasons for underperformance in certain classes:

- **Class Imbalance:** The dataset may have an uneven distribution of listings across price categories, leading to challenges in accurately predicting less-represented classes like 'Above Average.'

- **Feature Selection:** The features used for prediction might not capture all the nuances influencing price categories. Feature engineering and selection processes may require refinement to improve predictive power.

- **Complex Factors:** Pricing in the Airbnb marketplace can be influenced by complex factors beyond the scope of the current features, such as seasonality, local events, and market dynamics. These factors can contribute to classification difficulties.

Despite these challenges, the model offers valuable assistance to Airbnb management in pricing recommendations and a deeper understanding of the factors influencing price categories. Users can benefit from insights into expected price ranges based on their preferences, which can guide them in making informed booking decisions. Ongoing model refinement and feature engineering efforts hold the potential to enhance classification accuracy and address these limitations. 





## **c. Classification Tree Model**

*In the following section, the main objective is to construct a classification tree for predicting the cancellation policy of Airbnb listings. The classes of interest are "flexible," "moderate," and "strict." To determine the optimal tree size, cross-validation techniques will be applied. Subsequently, the rpart.plot library will be employed, with customized graphical parameters, to visually present the classification tree model, providing insights into the factors influencing Airbnb cancellation policies in New York City.*


### **Data Preprocessing**

```{r}
ny_ct.df <- ny.df
```


```{r}
# Subset data (remove unnecessary columns)
ny_ct.df <- subset(ny_ct.df, select= - c(id, amenities, bed_type, cleaning_fee, city, description, first_review, host_since,instant_bookable, last_review, latitude, longitude, name, thumbnail_url, neighbourhood, property_group, borough))
```


```{r}
# Inspect new dataset
str(ny_ct.df)
```


```{r}
# Convert character variables to factors
ny_ct.df$property_type <- as.factor(ny_ct.df$property_type)
ny_ct.df$room_type <- as.factor(ny_ct.df$room_type)
ny_ct.df$host_has_profile_pic <- factor(ny_ct.df$host_has_profile_pic, levels = c("t", "f"), labels = c("True", "False"))

str(ny_ct.df) # reinspect dataset
```
Note: 'cancellation' & 'host_identity_verified' already a factor variable & we do not need to modify any levels yet.


```{r}
# Change levels to "strict","moderate", & "flexible"
levels(ny_ct.df$cancellation_policy)[levels(ny_ct.df$cancellation_policy) == "strict"] <- "strict"
levels(ny_ct.df$cancellation_policy)[levels(ny_ct.df$cancellation_policy) == "super_strict"] <- "strict"
levels(ny_ct.df$cancellation_policy)[levels(ny_ct.df$cancellation_policy) == "flexible"] <- "flexible"
levels(ny_ct.df$cancellation_policy)[levels(ny_ct.df$cancellation_policy) == "moderate"] <- "moderate"
```


```{r}
#Partition data into training & validation sets
set.seed(92)
ny_ct.df_train.index <- sample(c(1:nrow(ny_ct.df)), nrow(ny_ct.df)*0.6)
ny_ct_train.df <- ny_ct.df[ny_ct.df_train.index, ]
ny_ct_valid.df <- ny_ct.df[-ny_ct.df_train.index, ]
```


```{r}
#Build the classification tree model
ct <- rpart(cancellation_policy~., ny_ct_train.df, method="class", xval= 10)
```


```{r}
# Determine the ideal tree size using Cross-validation
printcp(ct)
```


```{r}
# Determine the ideal tree size using Cross-validation
plotcp(ct)

# Keep the tree size where the cp value has the smallest error
ct_pruned <- prune(ct, 
                  cp = ct$cptable[which.min(ct$cptable[, "xerror"]), "CP"])

```


```{r}
# Plot the pruned tree
rpart.plot(ct_pruned, yesno = TRUE)
```

### **Explanation of Classification Tree:**

The Classification Tree model we constructed serves the purpose of predicting Airbnb hosts' cancellation policies in New York City, a critical aspect for both hosts and guests to understand. Our journey began with a meticulous phase of data preparation, including the removal of redundant columns, data type conversions, and handling of missing values. To simplify the classification task, we consolidated two levels of the "cancellation_policy" variable into the broader "strict" category. Subsequently, we partitioned the dataset into two distinct sets: a training set (comprising 60% of the data) and a validation set (comprising 40%), ensuring adequate representation of both cancellation policy types.

The construction of the decision tree model was an iterative process, involving the exploration of potential features that might influence cancellation policies. After thorough analysis, two key variables emerged as significant contributors: the number of reviews and the log price. These variables play a crucial role in understanding and predicting cancellation policies for Airbnb listings in New York City.

* *Guest and Host Experience*: The number of reviews can be seen as a proxy for the level of experience both hosts and guests have had with a particular listing. Listings with a high number of reviews may indicate a history of positive experiences, while those with fewer reviews might be relatively new or less frequently booked. Guests and hosts may have different expectations and behaviors depending on the listing's review history.

* *Trust and Credibility*: High review counts can contribute to building trust and credibility among potential guests. Hosts who maintain positive reviews are likely to have a more favorable cancellation policy, as they may want to uphold their reputation and maintain high occupancy rates. On the other hand, hosts with fewer reviews may adopt stricter policies to mitigate potential risks.

* *Price Sensitivity*: The price of a listing is a critical factor for both guests and hosts. Higher-priced listings may have more stringent cancellation policies to protect against last-minute cancellations that could result in significant revenue loss. Lower-priced listings, on the other hand, might offer more flexible cancellation options to attract cost-conscious guests.

* *Market Competition*: The pricing strategy of a listing could be influenced by the competitive landscape in the Airbnb market in New York City. Listings in highly competitive areas might offer more flexible cancellation policies to attract bookings, while those in less competitive areas may rely on stricter policies to secure confirmed reservations.

* *Guest Preferences*: Different guests may have varying levels of price sensitivity and risk tolerance. Some guests may prioritize flexibility in their travel plans and be willing to pay more for it, while others may prioritize cost savings and be less concerned about the cancellation policy. Hosts may adjust their pricing and policies to align with the preferences of their target guest demographic.

* *Seasonal Variations*: The importance of price and the number of reviews in predicting cancellation policies may vary seasonally. For example, during peak tourist seasons, hosts may increase prices and tighten cancellation policies to capitalize on high demand, while off-peak seasons may see lower prices and more lenient cancellation options.

By considering these factors, we created a decision tree model that effectively captures the dynamics of Airbnb rental cancellation policies in New York City. This model serves as a valuable tool for understanding the interplay of guest and host behavior, pricing strategies, and market conditions, benefiting both hosts and guests in the city's Airbnb ecosystem. It empowers hosts to make informed decisions about their cancellation policies, taking into account various factors that influence their listing's attractiveness to potential guests. Likewise, guests can use this model to better predict the cancellation policies they might encounter when booking an Airbnb in New York City, enabling them to make travel plans with confidence.





# **Clustering - Comparison of Neighborhoods in NYC**

*k-Means Analysis is applied in an effort to compare the Brooklyn neighborhoods in NYC.*

### **Data Pre-Processing**
```{r}
# Subset Brooklyn neighbors to be used as labels
ny_cluster.df <- subset(ny.df, borough == "Brooklyn")
```


```{r}
# Create new variable to combine 'number_of_reviews' & 'review_scores_rating'
ny_cluster.df <- ny_cluster.df %>%
  mutate(avg_review_scores_rating = review_scores_rating/number_of_reviews) 
```


```{r}
# Remove unnecessary columns
ny_cluster.df <- subset(ny_cluster.df, select= -c(id, property_type, room_type, amenities, bed_type, cancellation_policy, cleaning_fee, city, description, first_review, host_has_profile_pic, host_identity_verified, host_response_rate, host_since, instant_bookable, last_review, latitude, longitude, name, number_of_reviews, review_scores_rating, thumbnail_url, bedrooms, beds, borough, property_group))
```


```{r}
# Handle missing values
ny_cluster.df <- na.omit(ny_cluster.df)
```


```{r}
str(ny_cluster.df)  # Reinspect dataframe
```


```{r}
# Prepare data
cluster_labels = ny_cluster.df$neighbourhood
feature_var <- select(ny_cluster.df, -neighbourhood)

# Scale/standardize data to a mean of 0 & standard deviation of 1
df.scale <- scale(feature_var)
```



### **Determine optimal number of clusters (k)**
```{r}
# Compute distance between observations
ny_cluster.df.dist <- dist(df.scale)

# Determine 'k' value (# of clusters) using within sum squares
fviz_nbclust(df.scale, kmeans, method="wss") + labs(subtitle = "Elbow method")
```


### **k-Means Clustering**
```{r}
# k-means
optimal_k <- 4
km.out <- kmeans(df.scale, centers = optimal_k, nstart = 100)
```


### **Cluster Visualization/Interpretation**
```{r}
fviz_cluster(km.out, data = feature_var, stand = FALSE,
             geom = "point", ellipse.type = "convex", 
             main = "K-Means Clustering of Brooklyn Neighborhoods")
```


```{r}
# Generate table with cluster assignments
table(km.out$cluster, ny_cluster.df$neighbourhood)
```


```{r}
# Determine variable means for each cluster in the original metric (i.e., kmeans model output is based on standardized data)
aggregate(feature_var, by= list(cluster= km.out$cluster), mean)
```


Create boxplots for each variable (i.e., log_price, bathrooms, host_response_rate, latitude, longitude, review_scores_rating) by cluster to understand the distribution of data within each cluster:
```{r}
# Create a data frame with cluster labels
ny_cluster.df$cluster <- as.factor(km.out$cluster)
```


```{r}
# Boxplots for log_price by cluster
ggplot(ny_cluster.df, aes(x = cluster, y = log_price)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "log_price") +
  ggtitle("Boxplot of log_price by Cluster")
```


```{r}
# Boxplots for accommodates by cluster
ggplot(ny_cluster.df, aes(x = cluster, y = accommodates)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "accommodates") +
  ggtitle("Boxplot of accommodates by Cluster")
```


```{r}
# Boxplots for bathrooms by cluster
ggplot(ny_cluster.df, aes(x = cluster, y = bathrooms)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "bathrooms") +
  ggtitle("Boxplot of bathrooms by Cluster")
```


```{r}
# Boxplots for amenities_count by cluster
ggplot(ny_cluster.df, aes(x = cluster, y = amenities_count)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "amenities_count") +
  ggtitle("Boxplot of amenities_count by Cluster")
```


```{r}
# Boxplots for avg_review_scores_rating by cluster
ggplot(ny_cluster.df, aes(x = cluster, y = avg_review_scores_rating)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "avg_review_scores_rating") +
  ggtitle("Boxplot of avg_review_scores_rating by Cluster")
```




### **Explanation of k-Means Clustering Model:**

In the analysis of Brooklyn neighborhoods in New York City using k-Means clustering, several key steps were undertaken to uncover distinct clusters based on selected features. Initially, the data was pre-processed by narrowing it down to exclusively include Brooklyn neighborhoods and removing irrelevant columns and rows with missing values to ensure data quality. Additionally, we created a new feature, 'avg_review_scores_rating', which captures the quality of reviews more effectively by normalizing 'review_scores_rating' by 'number_of_reviews'. The features were then standardized to have a mean of 0 and a standard deviation of 1, ensuring equal influence of each variable in the clustering process (i.e., by preventing variables with larger scales from dominating the results). 

The optimal number of clusters (k) was determined using the "elbow method," resulting in the selection of k=4 as the most suitable choice. k-Means clustering was executed with 100 different starting configurations to enhance the likelihood of finding a globally optimal solution. Visualizing the clusters was facilitated through scatterplots, where each neighborhood was represented by a point, and convex ellipses delineated the clusters. Additionally, a table was generated to illustrate the assignment of neighborhoods to clusters.

To gain a deeper understanding of each cluster's characteristics, the means of selected variables (log_price, accommodates, bathrooms, amenities_count, and avg_review_scores_rating) were computed in their original metrics. The analysis revealed four distinct clusters of Brooklyn neighborhoods based on the selected features:

* *Luxury Living (Cluster 1):* This cluster represents neighborhoods characterized by higher prices, a greater number of bathrooms, higher average review scores, larger accommodation capacities, and a wealth of amenities.

* *Bare-Bone Bargains (Cluster 2):* Neighborhoods in this cluster are distinguished by lower prices, fewer bathrooms, lower average review scores, smaller accommodation capacities, and limited amenities.

* *Classic Comfort (Cluster 3):* This cluster comprises neighborhoods with moderate prices, a moderate number of bathrooms, moderately low average review scores, a moderate accommodation capacity, and a moderate level of amenities.

* *Your Average Joes (Cluster 4):* Neighborhoods in this cluster feature moderately low prices, fewer bathrooms, moderately high average review scores, a moderate accommodation capacity, and a limited number of amenities.

In conclusion, the k-Means clustering analysis helped identify and group Brooklyn neighborhoods in New York City based on common characteristics. The distinct clusters can serve as a valuable resource for property investors, tourists, or urban planners, facilitating informed decision-making concerning Brooklyn's various neighborhoods and their unique attributes.





# **Conclusions**

In conclusion, this project has provided valuable insights into the Airbnb rental properties in New York City through the application of various machine learning and data mining techniques. The process and findings of this analysis hold significant implications and potential benefits for a diverse range of stakeholders.

1. *Potential Benefits for Airbnb:* Airbnb, as a platform, can utilize the insights gained from this data mining project to enhance its recommendation system. By clustering neighborhoods based on similar listing characteristics, Airbnb can offer more personalized recommendations to customers. When a customer views a listing in one neighborhood, the system can suggest listings in other neighborhoods within the same cluster, thereby improving user experience and increasing bookings.

2. *Property Owners and Managers:* Property owners and managers can benefit from the regression analysis conducted in this project. They can use the predictive multiple linear regression model to evaluate and price their listings competitively within the overall NYC market or in specific neighborhoods. This data-driven pricing strategy can help optimize their revenue and occupancy rates.

3. *Real Estate Investors:* Real estate investors seeking opportunities in the short-term rental market can leverage the insights generated in this project. They can use the clustering and classification models to identify lucrative market segments and investment opportunities. Additionally, comparing insights from New York City to those of similar cities can inform investment decision-making for properties in other highly populated and frequented urban areas.

4. *Airbnb Customers:* Travelers and Airbnb customers can benefit from this data mining project by gaining access to information about reasonably priced listings. This can help them make informed decisions and ensure they are not overcharged for accommodations during their stay in New York City.

5. *Competitors in the Hospitality Industry:* Hotels and other competitors in the hospitality and travel industry can use the findings to better understand consumer preferences. Analyzing why consumers choose Airbnb over traditional hotel accommodations, such as lower nightly rates or higher accommodation capacities, can inform their marketing and service offerings to attract and retain customers.

6. *Market Researchers:* Professionals in the field of market research can use this data mining project as a reference to understand consumer behavior in the short-term rental market. It provides valuable insights into the factors that influence consumer preferences, helping businesses tailor their strategies accordingly.

7. *Policy Makers:* Government officials and regulators may find this data mining project useful for understanding the dynamics of the short-term rental market in New York City. The findings can inform policy decisions related to housing regulations, taxation, and tourism management.

In conclusion, the insights generated from this data mining project have the potential to benefit a wide range of entities, from individuals looking for affordable accommodations to businesses seeking to optimize their pricing strategies and policymakers aiming to make informed decisions about the short-term rental market. The information uncovered here is not only valuable for New York City but can also serve as a template for similar analyses in other highly populated and tourist-driven urban areas. The possibilities for leveraging these insights are indeed boundless across multiple sectors and industries.